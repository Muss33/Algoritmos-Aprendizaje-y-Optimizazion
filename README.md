# 游 Algoritmos de Aprendizaje e Optimizaci칩n en Frozen Lake

[![Estado del Proyecto](https://img.shields.io/badge/Estado-Finalizado-success)](https://github.com/Muss33/Algoritmos-Aprendizaje-y-Optimizazion)
[![Lenguaje Principal](https://img.shields.io/badge/Python-3.x-blue)](https://www.python.org/)
[![Entorno RL](https://img.shields.io/badge/Gymnasium-FrozenLake-green)](https://gymnasium.farama.org/)

---

##  1. Resumen y Objetivos

Este proyecto representa una inmersi칩n profunda en la implementaci칩n y el an치lisis comparativo de algoritmos fundamentales de la Inteligencia Artificial: **Aprendizaje por Refuerzo (RL)** y **Algoritmos Gen칠ticos (AG)**.

El campo de batalla elegido es el entorno estoc치stico **Frozen Lake** de Gymnasium (OpenAI Gym), un escenario ideal para evaluar c칩mo diferentes estrategias de aprendizaje gestionan la incertidumbre.

### Objetivos Clave:

- **Implementaci칩n Fiel:** Desarrollar las versiones *on-policy* (SARSA) y *off-policy* (Q-Learning) para observar su comportamiento.
- **An치lisis de Retornos:** Implementar Monte Carlo para comparar la actualizaci칩n basada en el retorno final frente a la diferencia temporal (TD).
- **Exploraci칩n de Optimizaci칩n:** Aplicar un Algoritmo Gen칠tico para contrastar el aprendizaje basado en recompensas con la optimizaci칩n evolutiva de pol칤ticas.
- **Influencia de Hiperpar치metros:** Determinar y documentar c칩mo los par치metros ($\alpha, \gamma, \epsilon$, tasa de mutaci칩n, etc.) afectan la convergencia y estabilidad de la soluci칩n.

---

##  2. El Entorno Frozen Lake

Frozen Lake es un juego de cuadr칤cula donde un agente (Silla) debe navegar hacia una meta (Regalo) evitando caer en agujeros (Lagos congelados).

| S칤mbolo | Significado |
| :--- | :--- |
| **S** | Inicio (Start) |
| **F** | Suelo Congelado (Frozen) |
| **H** | Agujero/Lagos (Hole) |
| **G** | Meta (Goal) |

> ** Caracter칤stica Cr칤tica: Estocasticidad**  
> La propiedad `is_slippery=True` introduce un desaf칤o crucial: al intentar moverse en una direcci칩n, el agente solo tiene una **probabilidad** de moverse en esa direcci칩n y una probabilidad de deslizarse a una de las dos direcciones adyacentes. Esto obliga a los algoritmos a encontrar pol칤ticas robustas, no solo un camino fijo.

---

##  3. Algoritmos en Detalle

### Aprendizaje por Refuerzo (RL)

| Algoritmo | Mecanismo de Actualizaci칩n | Foco Principal | Notas en el Informe |
| :--- | :--- | :--- | :--- |
| **Q-Learning** | Diferencia Temporal (TD) | **Explotaci칩n** (*Off-Policy*) | Se analiza su superioridad en la b칰squeda del valor 칩ptimo $Q^*$. |
| **SARSA** | Diferencia Temporal (TD) | **Exploraci칩n** (*On-Policy*) | Se compara su curva de aprendizaje m치s conservadora, siguiendo la pol칤tica actual. |
| **Monte Carlo** | Retorno Total del Episodio | **Exploraci칩n** (Promedio) | Necesita episodios completos; se documenta su lenta convergencia inicial. |

### Algoritmo Gen칠tico (AG)

- **Representaci칩n:** Cada individuo es un array que codifica una pol칤tica completa para el mapa ($16 \times 4$ estados).  
- **Funci칩n Fitness:** Evaluaci칩n basada en el porcentaje de 칠xitos en un n칰mero fijo de partidas.  
- **Mecanismos:** Se incluye la gesti칩n de **individuos 칠lite** para la preservaci칩n de las mejores soluciones.

---

##  4. An치lisis de Resultados

El informe (`informe_practica_ia.pdf`) contiene una secci칩n exhaustiva de resultados con:

### A. Gr치ficas de Rendimiento

Se muestran comparativas de la probabilidad de 칠xito promedio, destacando:

- La inestabilidad y el sobreaprendizaje con un $\alpha$ alto.  
- El efecto de la "miop칤a" del agente con un $\gamma$ bajo.  
- El balance entre $\epsilon$ y la capacidad de escapar de m치ximos locales.

### B. Comparativa Global

Se presenta una tabla y gr치ficas comparando:

- **Tasa de 칄xito Media:** Rendimiento final de las pol칤ticas 칩ptimas de cada algoritmo.  
- **Tiempo de Entrenamiento:** An치lisis de la eficiencia temporal, donde el Algoritmo Gen칠tico se dispara debido a la complejidad de su funci칩n *fitness*.

> **Conclusi칩n Clave del Informe:** El estudio demuestra que **Q-Learning** tiende a converger m치s r치pido a una pol칤tica *greedy* m치s efectiva, mientras que la baja escalabilidad del Algoritmo Gen칠tico lo limita severamente para problemas de mayor tama침o.

---

##  5. Estructura y Ejecuci칩n

###  Estructura del Repositorio

| Archivo/Directorio | Descripci칩n |
| :--- | :--- |
| `MonteCarlo.py` | Implementaci칩n del algoritmo Monte Carlo. |
| `Sarsa.py` | Implementaci칩n del algoritmo SARSA. |
| `QLearning.py` | Implementaci칩n del algoritmo Q-Learning. |
| `genetico.py` | Implementaci칩n del Algoritmo Gen칠tico. |
| `Evaluar.py` | Script para evaluar pol칤ticas generadas por los algoritmos. |
| `__main__.py` | Script de ejecuci칩n principal. |



###  Requisitos y Ejecuci칩n

Para replicar los resultados, aseg칰rate de tener instalado **Python 3.x** y las siguientes librer칤as:

```
pip install numpy gymnasium matplotlib

```

Para la ejecuci칩n del c칩digo, desc치rgate el repositorio y ejecuta el archivo ```__main__.py```


Autores: Alejandro Mart칤nez Hermosa, Mart칤n Serra Rubio.
